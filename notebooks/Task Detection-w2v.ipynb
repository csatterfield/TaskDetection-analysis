{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load(\"../models/normalized.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, time\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import textrank\n",
    "import os\n",
    "import sys\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rake_nltk import Rake\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "from math import log, floor\n",
    "import numpy as np\n",
    "import textrank\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, precision_recall_fscore_support\n",
    "import pickle\n",
    "import os\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.extend([\"chris\", \"satterfield\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(dateobj, start, end):\n",
    "    return dateobj >= start and dateobj < end        \n",
    "\n",
    "def tfidf(task_counters):\n",
    "    idf_counters = []\n",
    "    for counter in task_counters:\n",
    "        n = Counter()\n",
    "        for key in counter:\n",
    "            n[key] = 1\n",
    "        idf_counters.append(n)\n",
    "    total_counter = sum(idf_counters, Counter())\n",
    "    \n",
    "    weighted_counters = []\n",
    "    for counter in task_counters:\n",
    "        occurances = []\n",
    "        for key in counter.keys():\n",
    "            occurances.append(total_counter[key])\n",
    "        \n",
    "        mean = np.mean(occurances)\n",
    "        \n",
    "        weighted = {}\n",
    "        for key in counter.keys():\n",
    "            weighted[key] = counter[key]/((total_counter[key])  * (1 + abs(mean - total_counter[key])))\n",
    "        weighted_counters.append(Counter(weighted))\n",
    "    return weighted_counters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleDict(dictionary):\n",
    "    keys = list(dictionary.keys())\n",
    "    random.shuffle(keys)\n",
    "    shuffled = {}\n",
    "    for key in keys:\n",
    "        shuffled[key] = dictionary[key]\n",
    "    \n",
    "    return shuffled\n",
    "\n",
    "def rake(snapshot):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(snapshot)\n",
    "    keyphrases = r.get_ranked_phrases()\n",
    "    tokens = []\n",
    "    [tokens.extend(x.split(\" \")) for x in keyphrases]\n",
    "    return tokens\n",
    "\n",
    "def equals(prediction, expected):\n",
    "    if(prediction == expected):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_prediction(scores):\n",
    "    scores = shuffleDict(scores)\n",
    "    pred = max(scores, key=scores.get)\n",
    "    return pred\n",
    "\n",
    "def normalize_score(scores):\n",
    "    norm = np.linalg.norm(list(scores.values()))\n",
    "    for score in scores:\n",
    "        scores[score] = scores[score]/norm\n",
    "    return scores\n",
    "\n",
    "\n",
    "class ScreenshotTaskExtractor(object):\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "\n",
    "    def isProbablyEmail(self, task):\n",
    "        excludedWords = ['compose', 'gmail', 'inbox', 'google', 'starred', 'sent','mail','drafts','more','terms','privacy','program','policies']\n",
    "        i = 0\n",
    "        for word in excludedWords:\n",
    "            if word in task:\n",
    "                i += 1\n",
    "        return i > 4\n",
    "\n",
    "    def get_tasks_for_participant(self, path_to_data, participant, tokenizer, without_emails=True, ungrouped=False, using_tfidf=False):\n",
    "\n",
    "        with open(f\"{path_to_data}/{participant}/fulltext.pkl\", \"rb\") as f:\n",
    "            snapshotsWithDates = pickle.load(f)\n",
    "        \n",
    "        df = pd.read_excel(f\"{path_to_data}/{participant}/taskswitches_annotated.xlsx\")\n",
    "        offset = df[df[\"task\"] == \"offset\"][\"end\"].iloc[0]\n",
    "        df = df[df[\"task\"] != \"offset\"]\n",
    "        task_words_ungrouped = []\n",
    "        task_order = []\n",
    "\n",
    "        studyStartTime = snapshotsWithDates[0][0] - (datetime.combine(date.min, offset) - datetime.min)\n",
    "\n",
    "        for _,row in df.iterrows():\n",
    "            startDelta = datetime.combine(date.min, row[\"start\"]) - datetime.min \n",
    "            endDelta = datetime.combine(date.min, row[\"end\"]) - datetime.min\n",
    "            start = studyStartTime + startDelta\n",
    "            end = studyStartTime + endDelta\n",
    "            words = []\n",
    "\n",
    "\n",
    "            snapshotsInTask = [x[1].lower() for x in snapshotsWithDates if valid(x[0], start, end)]\n",
    "\n",
    "            if(without_emails):\n",
    "                snapshotsInTask = [x for x in snapshotsInTask if not self.isProbablyEmail(x)]\n",
    "\n",
    "            c = defaultdict(list)\n",
    "\n",
    "            for snapshot in snapshotsInTask:\n",
    "                tokens = tokenizer(snapshot)\n",
    "                snapshot_words = [x for x in tokens if x.isalpha()]\n",
    "                snapshot_words = [x for x in snapshot_words if not x in stop_words and x in self.vocab]\n",
    "                words.extend(snapshot_words)\n",
    "\n",
    "            if(len(words) > 0):\n",
    "                task_words_ungrouped.append(words)\n",
    "                task_order.append(row[\"task\"])\n",
    "        \n",
    "        task_counters = [Counter(x) for x in task_words_ungrouped]\n",
    "        if(not ungrouped):\n",
    "            counters = defaultdict(Counter)\n",
    "            for counter, task in zip(task_counters, task_order):\n",
    "                counters[task] += counter\n",
    "            task_order, task_counters = zip(*counters.items())\n",
    "            \n",
    "        if(using_tfidf):\n",
    "            task_counters = tfidf(task_counters)\n",
    "\n",
    "        task_vectors = []\n",
    "        for task in task_counters:\n",
    "            norm = np.linalg.norm(list(task.values()))\n",
    "            for word in task:\n",
    "                task[word] = task[word]/norm\n",
    "            v = np.zeros(300)\n",
    "            for word, score in task.most_common(100):\n",
    "                v += model[word] * task[word]\n",
    "            task_vectors.append(v)\n",
    "\n",
    "        return task_vectors, task_order\n",
    "    \n",
    "    \n",
    "def sim(v1, v2):\n",
    "    return cosine_similarity(v1.reshape(1,-1), v2.reshape(1,-1))[0][0]\n",
    "\n",
    "def create_results(method, predicted, expected, author, participant):\n",
    "    return [{\"method\": method, \"predicted\": x, \"expected\": y, \"author\": author, \"participant\": participant, \"correct\": equals(x, y)} for x,y in zip(predicted, expected)]\n",
    "prob = []\n",
    "\n",
    "def predict_simple(tasks, order, phrases):\n",
    "    predictions = []\n",
    "    expected = []\n",
    "    durations = []\n",
    "    sim_vocab = []\n",
    "\n",
    "    for task, actual in zip(tasks, order):\n",
    "        expected.append(actual)\n",
    "\n",
    "        scores = dict()\n",
    "        cover_scores = dict()\n",
    "        words = []\n",
    "        cover = {}\n",
    "        \n",
    "        for _, row in phrases.iterrows():\n",
    "            search_terms = word_tokenize(row[\"phrase\"])\n",
    "            search_terms = [x.lower() for x in search_terms if not x.lower() in stop_words]\n",
    "            search_terms = [x for x in search_terms if x in model]\n",
    "\n",
    "            v = np.zeros(300)\n",
    "            for word in search_terms:\n",
    "                v += model[word]\n",
    "            \n",
    "            scores[row[\"expected\"]] = sim(v, task)\n",
    "        \n",
    "        predictions.append(get_prediction(scores))\n",
    "\n",
    "    return predictions, expected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START ----- \n",
      "P01\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P02\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P03\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P04\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P05\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P06\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P07\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P08\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P11\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P12\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P13\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P14\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P15\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P16\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P17\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P18\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "START ----- \n",
      "P19\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "END -----\n",
      "0.40319444444444447\n",
      "(array([0.66498316, 0.22958694, 0.30976096, 0.45325444, 0.84010485,\n",
      "       0.42290749]), array([0.31854839, 0.38548387, 0.5016129 , 0.41630435, 0.47132353,\n",
      "       0.32      ]), array([0.43075245, 0.28777845, 0.38300493, 0.43399433, 0.60386246,\n",
      "       0.36432638]), array([1240, 1240, 1240,  920, 1360, 1200]))\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "df = pd.read_excel(\"../phrases_medium_and_high_v2.xlsx\")    \n",
    "results = []\n",
    "participants = [\"P01\", \"P02\", \"P03\", \"P04\", \"P05\", \"P06\", \"P07\", \"P08\", \"P11\", \"P12\", \"P13\", \"P14\", \"P15\", \"P16\", \"P17\", \"P18\", \"P19\"]\n",
    "path_to_data = \"../../archives/\"\n",
    "task_extractor = ScreenshotTaskExtractor(model.wv.vocab)\n",
    "\n",
    "\n",
    "for participant in participants:\n",
    "    print(\"START ----- \")\n",
    "    print(participant)\n",
    "    \n",
    "    tasks, order = task_extractor.get_tasks_for_participant(path_to_data, participant, word_tokenize, ungrouped=True)\n",
    "   \n",
    "    for author in df[\"author\"].unique():\n",
    "        task_descriptions = df[df[\"author\"] == author]\n",
    "        #print(\"Matching phrases by author: \" + author)\n",
    "        predicted,expected = predict_simple(tasks, order, task_descriptions)\n",
    "        r = create_results(\"w2v\", predicted, expected, author, participant)\n",
    "        results.extend(r)\n",
    "    \n",
    "    tasks, order = task_extractor.get_tasks_for_participant(path_to_data, participant, rake, ungrouped=True)\n",
    "    for author in df[\"author\"].unique():\n",
    "        task_descriptions = df[df[\"author\"] == author]\n",
    "        #print(\"Matching phrases by author: \" + author)\n",
    "        predicted,expected = predict_simple(tasks, order, task_descriptions)\n",
    "        r = create_results(\"w2v_rake\", predicted, expected, author, participant)\n",
    "        results.extend(r)\n",
    "\n",
    "        print(\"END -----\")\n",
    "    \n",
    "\n",
    "y_true = [x[\"expected\"] for x in results]\n",
    "y_pred = [x[\"predicted\"] for x in results]\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "print(precision_recall_fscore_support(y_true, y_pred))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_excel(\"matching_w2v.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
