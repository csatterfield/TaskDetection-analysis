{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import textrank\n",
    "import os\n",
    "import sys\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rake_nltk import Rake\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "from math import log, floor\n",
    "import numpy as np\n",
    "import textrank\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import pickle\n",
    "import os\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.extend([\"chris\", \"satterfield\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counter = None\n",
    "\n",
    "def tfidf_cosine_similarity(task, description):\n",
    "    global total_counter\n",
    "    c = Counter(description)\n",
    "    for key in c:\n",
    "        if key in total_counter:\n",
    "            c[key] = c[key]/total_counter[key]\n",
    "            \n",
    "    v1 = []\n",
    "    v2 = []\n",
    "    for key in total_counter:\n",
    "        if key in task:\n",
    "            v1.append(task[key])\n",
    "        else:\n",
    "            v1.append(0)\n",
    "            \n",
    "        if key in c:\n",
    "            v2.append(c[key])\n",
    "        else:\n",
    "            v2.append(0)\n",
    "            \n",
    "            \n",
    "    return sim(np.array(v1), np.array(v2))\n",
    "    \n",
    "    \n",
    "\n",
    "def rake(snapshot):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(snapshot)\n",
    "    keyphrases = r.get_ranked_phrases()\n",
    "    tokens = []\n",
    "    [tokens.extend(x.split(\" \")) for x in keyphrases]\n",
    "    return tokens\n",
    "\n",
    "def valid(dateobj, start, end):\n",
    "    return dateobj >= start and dateobj < end        \n",
    "\n",
    "def tfidf(task_counters, weighted_df):\n",
    "    global total_counter\n",
    "    idf_counters = []\n",
    "    for counter in task_counters:\n",
    "        n = Counter()\n",
    "        for key in counter:\n",
    "            n[key] = 1\n",
    "        idf_counters.append(n)\n",
    "    total_counter = sum(idf_counters, Counter())\n",
    "    \n",
    "    weighted_counters = []\n",
    "    for counter in task_counters:\n",
    "        occurances = []\n",
    "        for key in counter.keys():\n",
    "            occurances.append(total_counter[key])\n",
    "        \n",
    "        mean = np.mean(occurances)\n",
    "        \n",
    "        weighted = {}\n",
    "        for key in counter.keys():\n",
    "            if(weighted_df):\n",
    "                weighted[key] = counter[key]/((total_counter[key])  * (1 + abs(mean - total_counter[key])))\n",
    "            else:\n",
    "                weighted[key] = counter[key] /(total_counter[key])\n",
    "        weighted_counters.append(Counter(weighted))\n",
    "    return weighted_counters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffleDict(dictionary):\n",
    "    keys = list(dictionary.keys())\n",
    "    random.shuffle(keys)\n",
    "    shuffled = {}\n",
    "    for key in keys:\n",
    "        shuffled[key] = dictionary[key]\n",
    "    \n",
    "    return shuffled\n",
    "\n",
    "def equals(prediction, expected):\n",
    "    if(prediction == expected):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_prediction(scores):\n",
    "    scores = shuffleDict(scores)\n",
    "    pred = max(scores, key=scores.get)\n",
    "    #choices = list(range(1,len(scores) + 1))\n",
    "    #choices.append(-1)\n",
    "    #return np.random.choice(choices)\n",
    "    #if(scores[pred] < 0.05):\n",
    "    #    return -1\n",
    "    #else:\n",
    "    return pred\n",
    "\n",
    "def normalize_score(scores):\n",
    "    norm = np.linalg.norm(list(scores.values()), ord=1)\n",
    "    for score in scores:\n",
    "        if(norm != 0):\n",
    "            scores[score] = scores[score] / norm\n",
    "    return scores\n",
    "\n",
    "class WindowTitleTaskExtractor:\n",
    "\n",
    "    def get_tasks_for_participant(self, path_to_data, participant, tokenizer, weighted=False, without_emails=True, using_tfidf=False, ungrouped=False, widf=False):\n",
    "        \n",
    "        appdata = pd.read_csv(f\"{path_to_data}/{participant}/appdata_fixed.csv\")\n",
    "        df = pd.read_excel(f\"{path_to_data}/{participant}/taskswitches_annotated.xlsx\")\n",
    "        offset = df[df[\"task\"] == \"offset\"][\"end\"].iloc[0]\n",
    "        df = df[df[\"task\"] != \"offset\"]\n",
    "        task_words_ungrouped = []\n",
    "        task_order = []\n",
    "\n",
    "        studyStartTime = appdata[\"StartTime\"][0] - (datetime.combine(date.min, offset) - datetime.min).total_seconds()\n",
    "\n",
    "        for _,row in df.iterrows():\n",
    "            startDelta = datetime.combine(date.min, row[\"start\"]) - datetime.min\n",
    "            endDelta = datetime.combine(date.min, row[\"end\"]) - datetime.min\n",
    "            start = studyStartTime + startDelta.total_seconds()\n",
    "            end = studyStartTime + endDelta.total_seconds()\n",
    "\n",
    "            titles = appdata[((appdata[\"StartTime\"] >= start) & (appdata[\"StartTime\"] < end)) | ((appdata[\"EndTime\"] >= start) & (appdata[\"EndTime\"] < end))]\n",
    "            titles = list(titles[\"WindowTitle\"])\n",
    "            titles = [x.lower() for x in titles]\n",
    "\n",
    "            if(without_emails):\n",
    "                titles = [x for x in titles if not \"gmail\" in x]\n",
    "            \n",
    "            words = []\n",
    "            for title in titles:\n",
    "                tokens = word_tokenize(title)\n",
    "                snapshot_words = [x for x in tokens if len(x) > 2]\n",
    "                snapshot_words = [stemmer.stem(x) for x in snapshot_words if not x in stop_words]\n",
    "                words.extend(snapshot_words)\n",
    "\n",
    "            if(len(words) > 0):\n",
    "                task_words_ungrouped.append(words)\n",
    "                task_order.append(row[\"task\"])\n",
    "        \n",
    "        task_counters = [Counter(x) for x in task_words_ungrouped]\n",
    "        if(not ungrouped):\n",
    "            counters = defaultdict(Counter)\n",
    "            for counter, task in zip(task_counters, task_order):\n",
    "                counters[task] += counter\n",
    "            task_order, task_counters = zip(*counters.items())\n",
    "            \n",
    "        if(using_tfidf):\n",
    "            task_counters = tfidf(task_counters, weighted)\n",
    "\n",
    "        for task in task_counters:\n",
    "            norm = np.linalg.norm(list(task.values()))\n",
    "            for word in task:\n",
    "                task[word] = task[word]/norm\n",
    "\n",
    "        return task_counters, task_order\n",
    "\n",
    "            \n",
    "            \n",
    "class ScreenshotTaskExtractor(object):\n",
    "\n",
    "    def isProbablyEmail(self, task):\n",
    "        excludedWords = ['compose', 'gmail', 'inbox', 'google', 'starred', 'sent','mail','drafts','more','terms','privacy','program','policies']\n",
    "        i = 0\n",
    "        for word in excludedWords:\n",
    "            if word in task:\n",
    "                i += 1\n",
    "        return i > 4\n",
    "\n",
    "    def get_tasks_for_participant(self, path_to_data, participant, tokenizer, weighted=False, without_emails=True, ungrouped=False, using_tfidf=False):\n",
    "        with open(f\"{path_to_data}/{participant}/fulltext.pkl\", \"rb\") as f:\n",
    "            snapshotsWithDates = pickle.load(f)\n",
    "        \n",
    "        df = pd.read_excel(f\"{path_to_data}/{participant}/taskswitches_annotated.xlsx\")\n",
    "        offset = df[df[\"task\"] == \"offset\"][\"end\"].iloc[0]\n",
    "        startDelta = datetime.combine(date.min, offset) - datetime.min \n",
    "        df = df[df[\"task\"] != \"offset\"]\n",
    "        task_words_ungrouped = []\n",
    "        task_order = []\n",
    "        \n",
    "\n",
    "        studyStartTime = snapshotsWithDates[0][0] - (datetime.combine(date.min, offset) - datetime.min)\n",
    "        start = studyStartTime\n",
    "\n",
    "        for _,row in df.iterrows():\n",
    "            startDelta = datetime.combine(date.min, row[\"start\"]) - datetime.min \n",
    "            endDelta = datetime.combine(date.min, row[\"end\"]) - datetime.min\n",
    "            start = studyStartTime + startDelta\n",
    "            taskEnd = studyStartTime + endDelta\n",
    "            while(start < taskEnd):\n",
    "                end = start + timedelta(seconds=60)\n",
    "                snapshotsInTask = [x[1].lower() for x in snapshotsWithDates if valid(x[0], start, end)]\n",
    "                start = start + timedelta(seconds=60)\n",
    "\n",
    "                if(without_emails):\n",
    "                    snapshotsInTask = [x for x in snapshotsInTask if not self.isProbablyEmail(x)]\n",
    "\n",
    "                words = []\n",
    "                for snapshot in snapshotsInTask:\n",
    "                    tokens = tokenizer(snapshot)\n",
    "                    snapshot_words = [x for x in tokens if len(x) > 2]\n",
    "                    snapshot_words = [stemmer.stem(x) for x in snapshot_words if not x in stop_words]\n",
    "                    words.extend(snapshot_words)\n",
    "\n",
    "                if(len(words) > 0):\n",
    "                    task_words_ungrouped.append(words)\n",
    "                    task_order.append(row[\"task\"])\n",
    "        \n",
    "        task_counters = [Counter(x) for x in task_words_ungrouped]\n",
    "        if(not ungrouped):\n",
    "            counters = defaultdict(Counter)\n",
    "            for counter, task in zip(task_counters, task_order):\n",
    "                counters[task] += counter\n",
    "            task_order, task_counters = zip(*counters.items())\n",
    "            \n",
    "        if(using_tfidf):\n",
    "            task_counters = tfidf(task_counters, weighted)\n",
    "\n",
    "        for task in task_counters:\n",
    "            norm = np.linalg.norm(list(task.values()))\n",
    "            for word in task:\n",
    "                task[word] = task[word]/norm\n",
    "\n",
    "        return task_counters, task_order\n",
    "    \n",
    "    \n",
    "def sim(v1, v2):\n",
    "    return cosine_similarity(v1.reshape(1,-1), v2.reshape(1,-1))[0][0]\n",
    "\n",
    "def create_results(method, predicted, expected, author, participant):\n",
    "    return [{\"method\": method, \"predicted\": x, \"expected\": y, \"author\": author, \"participant\": participant, \"correct\": equals(x, y)} for x,y in zip(predicted, expected)]\n",
    "\n",
    "prob = []\n",
    "\n",
    "def predict_simple(tasks, order, phrases):\n",
    "    predictions = []\n",
    "    expected = []\n",
    "    durations = []\n",
    "    sim_vocab = []\n",
    "\n",
    "    for task, actual in zip(tasks, order):\n",
    "        expected.append(actual)\n",
    "\n",
    "        scores = dict()\n",
    "        cover_scores = dict()\n",
    "        words = []\n",
    "        cover = {}\n",
    "        \n",
    "        for _, row in phrases.iterrows():\n",
    "            search_terms = word_tokenize(row[\"phrase\"])\n",
    "            search_terms = [x.lower() for x in search_terms if not x.lower() in stop_words]\n",
    "            search_terms = [stemmer.stem(x) for x in search_terms]\n",
    "            \n",
    "            \n",
    "            scores[row[\"expected\"]] = tfidf_cosine_similarity(task, search_terms)\n",
    "\n",
    "        pred = get_prediction(scores)\n",
    "        predictions.append(pred)\n",
    "    return predictions, expected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START ----- \n",
      "P01\n",
      "END -----\n",
      "START ----- \n",
      "P02\n",
      "END -----\n",
      "START ----- \n",
      "P03\n",
      "END -----\n",
      "START ----- \n",
      "P04\n",
      "END -----\n",
      "START ----- \n",
      "P05\n",
      "END -----\n",
      "START ----- \n",
      "P06\n",
      "END -----\n",
      "START ----- \n",
      "P14\n",
      "END -----\n",
      "START ----- \n",
      "P15\n",
      "END -----\n",
      "START ----- \n",
      "P16\n",
      "END -----\n",
      "START ----- \n",
      "P17\n",
      "END -----\n",
      "START ----- \n",
      "P18\n",
      "END -----\n",
      "START ----- \n",
      "P19\n",
      "END -----\n",
      "0.5656185567010309\n",
      "(array([0.80331325, 0.47603834, 0.49485683, 0.35250241, 0.69043221,\n",
      "       0.68353414]), array([0.62900943, 0.61570248, 0.45177665, 0.59552846, 0.61887417,\n",
      "       0.5126506 ]), array([0.70555556, 0.53693694, 0.47233647, 0.44286578, 0.65269775,\n",
      "       0.5858864 ]), array([4240, 2420, 3940, 2460, 3020, 3320]))\n",
      "0.48308607381390956\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"../phrases_medium_and_high_v2.xlsx\")    \n",
    "results = []\n",
    "#participants = [\"P01\", \"P02\", \"P03\", \"P04\", \"P05\", \"P06\", \"P07\", \"P08\", \"P11\", \"P12\", \"P13\", \"P14\", \"P15\", \"P16\", \"P17\", \"P18\", \"P19\"]\n",
    "participants = [\"P01\", \"P02\", \"P03\", \"P04\", \"P05\", \"P06\", \"P14\", \"P15\", \"P16\", \"P17\", \"P18\", \"P19\"]\n",
    "\n",
    "results_tfidf = []\n",
    "results_tf = []\n",
    "results_rake = []\n",
    "results_rake_tfidf = []\n",
    "\n",
    "path_to_data = \"../../archives/\"\n",
    "task_extractor = ScreenshotTaskExtractor()\n",
    "\n",
    "\n",
    "for participant in participants:\n",
    "    print(\"START ----- \")\n",
    "    print(participant)\n",
    "    \n",
    "    \n",
    "    #if(participant in cache):\n",
    "    #    tasks,order = cache[participant]\n",
    "    #else:\n",
    "    #    tasks, order = task_extractor.get_tasks_for_participant(path_to_data, participant, word_tokenize, using_tfidf=True, ungrouped=True)\n",
    "    #    cache[participant] = (tasks, order)\n",
    "    \n",
    "    \n",
    "    tasks, order = task_extractor.get_tasks_for_participant(path_to_data, participant, word_tokenize, using_tfidf=True, ungrouped=True)\n",
    "\n",
    "    for author in df[\"author\"].unique():\n",
    "        task_descriptions = df[df[\"author\"] == author]\n",
    "        #print(\"Matching phrases by author: \" + author)\n",
    "        predicted,expected = predict_simple(tasks, order, task_descriptions)\n",
    "        r = create_results(\"tfidf\", predicted, expected, author, participant)\n",
    "        results.extend(r)\n",
    "    \n",
    "    '''\n",
    "    tasks, order = task_extractor.get_tasks_for_participant(path_to_data, participant, word_tokenize, using_tfidf=False, ungrouped=True)\n",
    "\n",
    "    for author in df[\"author\"].unique():\n",
    "        task_descriptions = df[df[\"author\"] == author]\n",
    "        #print(\"Matching phrases by author: \" + author)\n",
    "        predicted,expected = predict_simple(tasks, order, task_descriptions)\n",
    "        r = create_results(\"tf\", predicted, expected, author, participant)\n",
    "        results.extend(r)\n",
    "        \n",
    "    tasks, order = task_extractor.get_tasks_for_participant(path_to_data, participant, rake, using_tfidf=False, ungrouped=True)\n",
    "\n",
    "    for author in df[\"author\"].unique():\n",
    "        task_descriptions = df[df[\"author\"] == author]\n",
    "        #print(\"Matching phrases by author: \" + author)\n",
    "        predicted,expected = predict_simple(tasks, order, task_descriptions)\n",
    "        r = create_results(\"rake\", predicted, expected, author, participant)\n",
    "        results.extend(r)\n",
    "        \n",
    "    tasks, order = task_extractor.get_tasks_for_participant(path_to_data, participant, rake, using_tfidf=True, ungrouped=True)\n",
    "\n",
    "    for author in df[\"author\"].unique():\n",
    "        task_descriptions = df[df[\"author\"] == author]\n",
    "        #print(\"Matching phrases by author: \" + author)\n",
    "        predicted,expected = predict_simple(tasks, order, task_descriptions)\n",
    "        r = create_results(\"rake-tfidf\", predicted, expected, author, participant)\n",
    "        results.extend(r)\n",
    "    '''\n",
    "    print(\"END -----\")\n",
    "    \n",
    "y_true = [x[\"expected\"] for x in results]\n",
    "y_pred = [x[\"predicted\"] for x in results]\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "print(precision_recall_fscore_support(y_true, y_pred, labels=[1,2,3,4,5,6]))\n",
    "print(matthews_corrcoef(y_true, y_pred))\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"matching_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
