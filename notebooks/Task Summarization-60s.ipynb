{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import textrank\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stopwords = stopwords.words(\"english\")\n",
    "stopwords.extend([\"chris\", \"satterfield\", \"cds00\", \"cs\", \"ubc\", \"ca\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load(\"../models/normalized.model\")\n",
    "#nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(dateobj, start, end):\n",
    "    return dateobj >= start and dateobj < end        \n",
    "\n",
    "def tfidf(task_counters):\n",
    "    idf_counters = []\n",
    "    for counter in task_counters:\n",
    "        n = Counter()\n",
    "        for key in counter:\n",
    "            n[key] = 1\n",
    "        idf_counters.append(n)\n",
    "    total_counter = sum(idf_counters, Counter())\n",
    "    \n",
    "    weighted_counters = []\n",
    "    for counter in task_counters:\n",
    "        occurances = []\n",
    "        for key in counter.keys():\n",
    "            occurances.append(total_counter[key])\n",
    "        \n",
    "        mean = np.mean(occurances)\n",
    "        \n",
    "        weighted = {}\n",
    "        for key in counter.keys():\n",
    "            weighted[key] = counter[key]/((total_counter[key]) * (1 + abs(mean - total_counter[key])))\n",
    "        weighted_counters.append(Counter(weighted))\n",
    "    return weighted_counters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScreenshotTaskExtractor(object):\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "\n",
    "    def isProbablyEmail(self, task):\n",
    "        excludedWords = ['compose', 'gmail', 'inbox', 'google', 'starred', 'sent','mail','drafts','more','terms','privacy','program','policies']\n",
    "        i = 0\n",
    "        for word in excludedWords:\n",
    "            if word in task:\n",
    "                i += 1\n",
    "        return i > 4\n",
    "\n",
    "    def get_tasks_for_participant(self, path_to_data, participant, without_emails=True, ungrouped=False, filter=lambda x: word_tokenize(x), using_tfidf=False):\n",
    "\n",
    "        with open(f\"{path_to_data}/{participant}/fulltext.pkl\", \"rb\") as f:\n",
    "            snapshotsWithDates = pickle.load(f)\n",
    "        \n",
    "        df = pd.read_excel(f\"{path_to_data}/{participant}/taskswitches_annotated.xlsx\")\n",
    "        offset = df[df[\"task\"] == \"offset\"][\"end\"].iloc[0]\n",
    "        df = df[df[\"task\"] != \"offset\"]\n",
    "        task_words_ungrouped = []\n",
    "        task_order = []\n",
    "\n",
    "        studyStartTime = snapshotsWithDates[0][0] - (datetime.combine(date.min, offset) - datetime.min)\n",
    "\n",
    "        for _,row in df.iterrows():\n",
    "            startDelta = datetime.combine(date.min, row[\"start\"]) - datetime.min \n",
    "            endDelta = datetime.combine(date.min, row[\"end\"]) - datetime.min\n",
    "            start = studyStartTime + startDelta\n",
    "            end = studyStartTime + endDelta\n",
    "\n",
    "            snapshotsInTask = [x[1].lower() for x in snapshotsWithDates if valid(x[0], start, end)]\n",
    "            \n",
    "            if(without_emails):\n",
    "                snapshotsInTask = [x for x in snapshotsInTask if not self.isProbablyEmail(x)]\n",
    "\n",
    "            c = defaultdict(list)\n",
    "\n",
    "            for snapshot in snapshotsInTask:\n",
    "                tokens = word_tokenize(snapshot)\n",
    "                snapshot_words = [x for x in tokens if x.isalpha()]\n",
    "                snapshot_words = [x for x in snapshot_words if not x in stopwords and x in self.vocab]\n",
    "                snapshot_words = [x for x in snapshot_words if len(x) > 2]\n",
    "                stemmed_words = [(stemmer.stem(x), x) for x in snapshot_words]\n",
    "                \n",
    "                for stem, word in stemmed_words:\n",
    "                    c[stem].append(word)\n",
    "                \n",
    "            words = []\n",
    "            for stem in c:\n",
    "                counter  = Counter(c[stem])\n",
    "                words.extend([counter.most_common(1)[0][0]] * sum(counter.values()))\n",
    "\n",
    "            if(len(words) > 0):\n",
    "                task_words_ungrouped.append(words)\n",
    "                task_order.append(row[\"task\"])\n",
    "        \n",
    "        task_counters = [Counter(x) for x in task_words_ungrouped]\n",
    "        if(not ungrouped):\n",
    "            counters = defaultdict(Counter)\n",
    "            for counter, task in zip(task_counters, task_order):\n",
    "                counters[task] += counter\n",
    "            task_order, task_counters = zip(*counters.items())\n",
    "            \n",
    "        if(using_tfidf):\n",
    "            task_counters = tfidf(task_counters)\n",
    "\n",
    "        for task in task_counters:\n",
    "            norm = np.linalg.norm(list(task.values()))\n",
    "            for word in task:\n",
    "                task[word] = task[word]/norm\n",
    "\n",
    "        return task_counters, task_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = ScreenshotTaskExtractor(model.wv.vocab)\n",
    "path_to_data = \"../../archives/\"\n",
    "participants = [\"P01\", \"P02\", \"P03\", \"P04\",\n",
    "                \"P05\", \"P06\", \"P07\", \"P08\",\n",
    "                \"P11\", \"P12\", \"P13\", \"P14\",\n",
    "                \"P15\", \"P16\", \"P17\", \"P18\", \"P19\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTICIPANT:  P01\n",
      "--------------------------\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'datetime.time' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-25ac295a4010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mn_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tasks_for_participant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticipant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mungrouped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musing_tfidf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"../wordclouds/task segments weighted/{participant}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"../wordclouds/task segments weighted/{participant}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-012ddce2944b>\u001b[0m in \u001b[0;36mget_tasks_for_participant\u001b[0;34m(self, path_to_data, participant, without_emails, ungrouped, filter, using_tfidf)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mendDelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstartDelta\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudyStartTime\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstartDelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudyStartTime\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mendDelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'datetime.time' and 'int'"
     ]
    }
   ],
   "source": [
    "for participant in participants:\n",
    "    print(\"PARTICIPANT: \", participant)\n",
    "    print(\"--------------------------\")\n",
    "    print()\n",
    "    index = 0\n",
    "    \n",
    "    n_words = []\n",
    "    \n",
    "    tasks = te.get_tasks_for_participant(path_to_data, participant, ungrouped=True, using_tfidf=True)\n",
    "    if not os.path.exists(f\"../wordclouds/task segments weighted/{participant}\"):\n",
    "        os.mkdir(f\"../wordclouds/task segments weighted/{participant}\")\n",
    "    for task, order in zip(tasks[0], tasks[1]):\n",
    "        n_words.append(len(task.keys()))\n",
    "        print(len(task.keys()))\n",
    "        #print(order, task.most_common(10))\n",
    "        #wc = WordCloud(background_color=\"white\", max_words=50)\n",
    "        #wc.generate_from_frequencies(task)\n",
    "        #plt.imshow(wc, interpolation=\"bilinear\")\n",
    "        #plt.axis(\"off\")\n",
    "        #plt.savefig(f'../wordclouds/task segments weighted/{participant}/wc{index}_label_{order}.png', bbox_inches='tight')\n",
    "        #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1148.111111111111"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
